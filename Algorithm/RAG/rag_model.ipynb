{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\jincheng_project\\RAG\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     r\"D:\\Backend_insurance\\Algorithm\\Fine_tuning\\DeepSeek-R1-Distill-Llama-8B\", \n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     r\"D:\\Backend_insurance\\Algorithm\\Fine_tuning\\DeepSeek-R1-Distill-Llama-8B\", \n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     load_in_4bit=True\n",
    "# )\n",
    "\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1000)\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = OllamaLLM(model=\"deepseek-r1:8b\", base_url=\"http://localhost:11434\", temperature = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredExcelLoader(r\"D:\\jincheng_project\\RAG\\pension_complaints_rewritten.xlsx\", mode=\"elements\")\n",
    "do = loader.load()\n",
    "\n",
    "all_splits  = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True\n",
    "    ).split_documents(do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_40948\\2404468741.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=r\"D:\\jincheng_project\\RAG\\all-mpnet-base-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=r\"D:\\jincheng_project\\RAG\\all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Template = \"\"\"\n",
    "你是一个专业的养老保险索赔助理.请根据过去的经验直接回答用户的询问。答案必须准确。请使用专业语言，不要使用第一人称。如果有人询问电话号码，请提供正确答案。如果您不知道电话号码，请告诉我们您没有关于该号码的信息。如果有人询问并且涉及相关部门的名称，并且您有关于该部门的信息，请提及该部门。如果你没有资料, 请避免提供错误的名称。\n",
    "\n",
    "请记住，你是保险专家，如果有人问你的问题，这是完全不相关的，而不是试图根据保险上下文回答，你必须把它通用和一般的反应。\n",
    "\n",
    "例如:\n",
    "问：你好，你好吗 \n",
    "答：我很好，谢谢。\n",
    "这是通用的响应，你必须像我在示例中那样以通用的方式处理它\n",
    "\n",
    "上下文环境：\n",
    "{context}\n",
    "\n",
    "问题：\n",
    "{question}\n",
    "\n",
    "回答：\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(Template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"], k=1)\n",
    "    if not retrieved_docs:\n",
    "        return {\"context\": [Document(page_content=\"无相关信息\")]}\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    formatted_prompt = prompt.format(question=state[\"question\"], context=docs_content)\n",
    "    \n",
    "    # Get just the generated text without the prompt\n",
    "    response = llm(formatted_prompt)\n",
    "    \n",
    "    # Extract just the answer part (you may need to adjust this based on your model's output format)\n",
    "    if isinstance(response, dict):\n",
    "        answer = response.get(\"generated_text\", \"\").replace(formatted_prompt, \"\").strip()\n",
    "    else:\n",
    "        answer = response.strip()\n",
    "    \n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"你好， 朋友。 我 怎么了\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，我现在需要帮助用户解决一个关于养老金领取的问题。根据上下文，用户提到他们在高平市领取养老金，并且是在2024年4月18日通过民生山西APP进行资格认证时遇到了系统维护的问题，无法完成认证。\n",
      "\n",
      "首先，我应该确认用户的具体情况：他们是高平市的退休居民，使用了民生山西APP进行认证，但系统出现了问题。接下来，我需要提供解决方案，帮助用户处理这个问题。\n",
      "\n",
      "根据之前的经验，当遇到系统维护或无法完成在线认证时，最好的建议通常是耐心等待维护结束后再次尝试，或者通过其他渠道进行认证，比如电话联系相关部门或走访当地服务中心。这样可以确保用户能够顺利完成资格认证，并及时领取养老金。\n",
      "\n",
      "另外，我还应该提醒用户，如果在等待期间有任何问题，可以再次拨打客服热线进行咨询，这样他们可以获得更具体的指导和帮助。\n",
      "\n",
      "总结一下，我的回答应该包括以下几点：\n",
      "1. 建议用户耐心等待系统维护结束后，再次尝试通过民生山西APP进行认证。\n",
      "2. 提供其他可选的认证渠道，如电话联系相关部门或前往当地服务中心。\n",
      "3. 提醒用户如果仍有问题，可以拨打客服热线进一步咨询。\n",
      "\n",
      "这样既解决了当前的问题，又提供了多种解决方案，确保用户能够顺利完成养老金领取的过程。\n",
      "</think>\n",
      "\n",
      "您好。针对您在高平市领取养老金的情况，您可以尝试通过以下方式处理：\n",
      "\n",
      "1. **耐心等待**：系统维护结束后，请再次尝试通过民生山西APP进行认证。\n",
      "\n",
      "2. **多渠道认证**：如仍有问题，可考虑通过电话联系相关部门或前往当地养老服务中心进行走访认证。\n",
      "\n",
      "3. **进一步咨询**：如果仍有疑问，欢迎拨打相关部门的客服热线进行详细咨询和指导。\n",
      "\n",
      "建议您耐心等待并多渠道尝试，以便顺利完成资格认证。如有需要，可再次来电咨询以获取更具体信息。\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\jincheng_project\\RAG\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "class InsuranceModels:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        self.model = OllamaLLM(model=\"deepseek-r1:14b\", base_url=\"http://localhost:11434\")\n",
    "        self.embeddings = self.text_embedding()\n",
    "        self.vector_store = self.vector_search()\n",
    "        self.prompt = self.format_prompt()\n",
    "        self.all_splits = self.doc_splitting()\n",
    "        _ = self.vector_store.add_documents(documents=self.all_splits)\n",
    "        self.graph = self.load() \n",
    "    def doc_splitting(self):\n",
    "        loader = UnstructuredExcelLoader(r\"D:\\jincheng_project\\RAG\\pension_complaints_rewritten.xlsx\", mode=\"elements\")\n",
    "        do = loader.load()\n",
    "\n",
    "        all_splits  = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=400,\n",
    "                chunk_overlap=200,\n",
    "                add_start_index=True\n",
    "            ).split_documents(do)\n",
    "        return all_splits\n",
    "    def text_embedding(self):\n",
    "        return HuggingFaceEmbeddings(model_name=r\"D:\\jincheng_project\\RAG\\all-mpnet-base-v2\")\n",
    "    def vector_search(self):\n",
    "        embedding_dim = len(self.embeddings.embed_query(\"hello world\"))\n",
    "        index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "        vector_store = FAISS(\n",
    "            embedding_function=self.embeddings,\n",
    "            index=index,\n",
    "            docstore=InMemoryDocstore(),\n",
    "            index_to_docstore_id={},\n",
    "        )\n",
    "        return vector_store\n",
    "    \n",
    "    def retrieve(self, state: State):\n",
    "        retrieved_docs = self.vector_store.similarity_search(state[\"question\"], k=1)\n",
    "        if not retrieved_docs:\n",
    "            return {\"context\": [Document(page_content=\"无相关信息\")]}\n",
    "        return {\"context\": retrieved_docs}\n",
    "\n",
    "    def generate(self, state: State):\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "        formatted_prompt = self.prompt.format(question=state[\"question\"], context=docs_content)\n",
    "        \n",
    "        # Get just the generated text without the prompt\n",
    "        response = self.model(formatted_prompt)\n",
    "        \n",
    "        # Extract just the answer part (you may need to adjust this based on your model's output format)\n",
    "        if isinstance(response, dict):\n",
    "            answer = response.get(\"generated_text\", \"\").replace(formatted_prompt, \"\").strip()\n",
    "        else:\n",
    "            answer = response.strip()\n",
    "        \n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "\n",
    "    def format_prompt(self):\n",
    "        Template = \"\"\"\n",
    "            您是专业的养老保险理赔助理。请根据以往经验直接回答用户的咨询。答案必须准确。请使用专业语言，请勿使用第一人称。如果有人询问电话号码，请提供正确答案。如果您不知道电话号码，请告知您没有该号码的信息。\n",
    "            如果有人询问，并且涉及相关部门名称，并且您有该部门的信息，请提及该部门。如果您没有信息，请避免提供错误的名称。如果有人问一般的问题，你必须相应地回答\n",
    "\n",
    "            上下文：\n",
    "            {context}\n",
    "\n",
    "            问题：\n",
    "            {question}\n",
    "\n",
    "            答案：\n",
    "            \"\"\"\n",
    "\n",
    "        prompt = PromptTemplate.from_template(Template)\n",
    "        return prompt\n",
    "    def load(self):\n",
    "        # Compile application and test\n",
    "        graph_builder = StateGraph(State).add_sequence([self.retrieve, self.generate])\n",
    "        graph_builder.add_edge(START, \"retrieve\")\n",
    "        graph = graph_builder.compile()\n",
    "        return graph\n",
    "\n",
    "    def transcribe(self, question: str):\n",
    "        response = self.graph.invoke({\"question\": question})    \n",
    "        return response     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = InsuranceModels()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "关于您提到的养老保险费逾期导致罚款的问题，通常情况下，参保人员需要按照社会保险法规定的时间节点缴纳保险费用。如果您因故未能按时缴纳，可能会产生滞纳金。建议您尽快联系当地的社会保险经办机构或通过官方提供的线上平台查询您的账户状态，并了解具体的补缴流程和相关政策规定。\n"
     ]
    }
   ],
   "source": [
    "res = ins.transcribe(\"没有人告诉我逾期保险费，现在我有罚款。我做什么\")\n",
    "rr = str(res['answer']).split(\"</think>\")[-1]\n",
    "print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
